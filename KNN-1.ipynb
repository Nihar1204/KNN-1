{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Assignment-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. What is the KNN algorithm?\n",
    "\n",
    "# Ans:\n",
    "\n",
    "# The k-Nearest Neighbors (KNN) algorithm is a simple yet powerful supervised machine learning \n",
    "# algorithm used for both classification and regression tasks. It classifies a new data point \n",
    "# based on the majority class among its k-nearest neighbors in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. How do you choose the value of K in KNN?\n",
    "\n",
    "# Ans:\n",
    "\n",
    "# Rule of Thumb:\n",
    "# A common starting point is to set k = sqrt(n), where n is the number of data points in our training\n",
    "# set. This provides a balance between considering enough neighbors and avoiding excessive computation.\n",
    "\n",
    "# Cross-Validation:\n",
    "# This is a more robust approach. We can use techniques like k-fold cross-validation:\n",
    "# Divide our training data into k subsets (folds).\n",
    "# Train and evaluate our KNN model for different values of k, using each fold as a validation set \n",
    "# and the remaining folds as training data.\n",
    "# Calculate the average performance (e.g., accuracy, error rate) across all folds for each k value.\n",
    "# Choose the k value that gives the best average performance.\n",
    "\n",
    "# Elbow Method:\n",
    "# This method involves plotting the error rate (or accuracy) of the KNN model against different k values.\n",
    "# Look for an \"elbow\" point in the plot, where the error rate starts to flatten out. \n",
    "# This point often indicates a good balance between bias and variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. What is the difference between KNN classifier and KNN regressor?\n",
    "\n",
    "# Ans:\n",
    "\n",
    "# For classification problem we use KNN classifier. Whereas for a continuous outcome we use\n",
    "# KNN regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. How do you measure the performance of KNN?\n",
    "\n",
    "# Ans:\n",
    "\n",
    "# The way to measure the performance of a K-Nearest Neighbor (KNN) algorithm depends on\n",
    "# whether we're using it for classification or regression.\n",
    "\n",
    "# Classification:\n",
    "# Accuracy, Precision, Recall, F1-Score, Confusion Matrix, AUC, ROC Curve and Log Loss.\n",
    "\n",
    "# Regression:\n",
    "# MSE, RMSE, MAE, R-squared, Adjusted R-squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. What is the curse of dimensionality in KNN?\n",
    "\n",
    "# And:\n",
    "\n",
    "# The curse of dimensionality states, the difficulties that arise when dealing with high-dimensional \n",
    "# data. In the context of K-Nearest Neighbors (KNN), it refers to the phenomenon where the \n",
    "# performance of the algorithm degrades as the number of features (dimensions) increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6. How do you handle missing values in KNN?\n",
    "\n",
    "# Ans:\n",
    "\n",
    "# We can use imputation techniques like Mean/Median imputation, Mode imputation or we can use\n",
    "# K-Nearest Neighbors (KNN) imputation, because it is a more sophisticated approach where missing\n",
    "# values are imputed based on the values of their k-nearest neighbors. It leverages the relationships\n",
    "# between features to make more accurate imputations.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for\n",
    "# which type of problem?\n",
    "\n",
    "# KNN Classifier:\n",
    "# Output: Categorical label (class)\n",
    "# Prediction: Majority voting\n",
    "# Evaluation: Accuracy, precision, recall, F1-score, etc.\n",
    "# Problem Type: Classification\n",
    "\n",
    "# KNN Regressor:\n",
    "# Output: Continuous value\n",
    "# Prediction: Averaging\n",
    "# Evaluation: MSE, RMSE, MAE, R-squared\n",
    "# Problem Type: Regression\n",
    "\n",
    "# The choice between KNN Classifier and KNN Regressor depends on the nature of the data and the \n",
    "# problem we're trying to solve. If we're dealing with categorical data and need to classify data\n",
    "# points into categories, we need to use KNN Classifier. If we're dealing with continuous data and\n",
    "# need to predict a value, then we need to use KNN Regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks,\n",
    "# and how can these be addressed?\n",
    "\n",
    "# KNN Classifier\n",
    "\n",
    "# Strengths:\n",
    "\n",
    "# Simple and Intuitive: Easy to understand and implement. The logic is straightforward.\n",
    "# No Training Phase: KNN is a \"lazy learner.\" It doesn't build a model upfront, which can be \n",
    "# advantageous when training time is a constraint.\n",
    "# Versatile: Can be used for multi-class classification problems.\n",
    "# Adaptable: Easily adapts to new data. We can simply add new labeled data points to the existing\n",
    "# dataset without retraining.\n",
    "\n",
    "# Weaknesses:\n",
    "\n",
    "# Computationally Expensive: Prediction can be slow, especially with large datasets, as it requires\n",
    "# calculating distances to all training points.\n",
    "# Sensitive to Irrelevant Features: The presence of irrelevant features can negatively impact \n",
    "# performance. All features contribute equally to distance calculations, so irrelevant ones can skew results.\n",
    "# Sensitive to Data Scaling: Features with larger scales can dominate the distance calculations.\n",
    "# Optimal 'k' Value is Data-Dependent: Requires careful selection of the 'k' value. There's no \n",
    "# one-size-fits-all solution.\n",
    "# Susceptible to the Curse of Dimensionality: Performance degrades in high-dimensional spaces \n",
    "# due to data sparsity and distance concentration.\n",
    "# Bias towards Majority Class: In imbalanced datasets, KNN can be biased towards the majority class.\n",
    "\n",
    "# Addressing Weaknesses (Classifier):\n",
    "\n",
    "# Feature Selection/Engineering: Select the most relevant features and remove or transform irrelevant\n",
    "# ones. This reduces dimensionality and improves performance.\n",
    "# Dimensionality Reduction (PCA, LDA): Reduce the number of features while preserving important information.\n",
    "# Feature Scaling (Standardization, Normalization): Scale features to a similar range to prevent \n",
    "# features with larger values from dominating distance calculations.\n",
    "# Cross-Validation: Use techniques like k-fold cross-validation to find the optimal 'k' value.\n",
    "# Distance Metrics: Experiment with different distance metrics (e.g., Manhattan distance) to see\n",
    "# which works best for your data.\n",
    "# Weighted KNN: Assign weights to neighbors based on their distance. Closer neighbors have a \n",
    "# greater influence on the prediction. This can help address the bias towards majority classes.\n",
    "# Ensemble Methods: Combine multiple KNN models with different 'k' values or distance metrics \n",
    "# to improve overall performance.\n",
    "\n",
    "# KNN Regressor\n",
    "\n",
    "# Strengths:\n",
    "\n",
    "# Simple and Intuitive: Easy to understand and implement.\n",
    "# No Training Phase: Like the classifier, it's a lazy learner.\n",
    "# Adaptable: New data can be easily added.\n",
    "\n",
    "# Weaknesses:\n",
    "\n",
    "# Computationally Expensive: Prediction can be slow.\n",
    "# Sensitive to Irrelevant Features and Data Scaling: Similar to the classifier, these can skew results.\n",
    "# Optimal 'k' Value is Data-Dependent: Requires careful selection.\n",
    "# Susceptible to the Curse of Dimensionality: Performance degrades in high-dimensional spaces.\n",
    "# Sensitive to Outliers: Outliers can significantly influence the average value calculation.\n",
    "\n",
    "# Addressing Weaknesses (Regressor):\n",
    "\n",
    "# Feature Selection/Engineering: Crucial for improving performance.\n",
    "# Dimensionality Reduction: Helps with high-dimensional data.\n",
    "# Feature Scaling: Important for distance-based calculations.\n",
    "# Cross-Validation: Essential for finding the best 'k' value.\n",
    "# Robust Distance Metrics: Consider metrics less sensitive to outliers (e.g., Minkowski distance \n",
    "# with a higher power).\n",
    "# Weighted KNN: Give more weight to closer neighbors. This can help reduce the influence of outliers.\n",
    "# Data Cleaning/Outlier Removal: Identify and handle outliers in the data before applying KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?\n",
    "\n",
    "# Ans:\n",
    "\n",
    "# Euclidean distance: The \"straight-line\" distance between two points.\n",
    "# Manhattan distance: The \"city block\" distance between two points. It measures the distance\n",
    "# traveled along the axes of a coordinate system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q10. What is the role of feature scaling in KNN?\n",
    "\n",
    "# Ans:\n",
    "\n",
    "# KNN is a distance-based algorithm. It calculates the distance between data points to find \n",
    "# the \"nearest\" neighbors.   \n",
    "# Features with larger values can dominate the distance calculations, overshadowing the \n",
    "# influence of features with smaller values.\n",
    "\n",
    "# So, feature scaling is an essential preprocessing step for KNN. It ensures that all features \n",
    "# contribute equally to distance calculations, leading to more accurate and reliable neighbor \n",
    "# identification, and ultimately improving the performance of the KNN algorithm.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
